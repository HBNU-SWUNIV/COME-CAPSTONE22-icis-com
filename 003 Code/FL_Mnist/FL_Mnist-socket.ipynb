{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4252bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./FL_Mnist')\n",
    "from fl_mnist_implementation_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5911bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./FL_Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65aec097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# zipfile.ZipFile('Mnist.zip').extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0331e5b",
   "metadata": {},
   "source": [
    "# 패키지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0268a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from fl_mnist_implementation_tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9c107",
   "metadata": {},
   "source": [
    "# MNIST 데이터 세트 읽기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fd5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(paths, verbose=-1):\n",
    "    '''expects images for each class in seperate dir, \n",
    "    e.g all digits in 0 class in the directory named 0 '''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f68c13",
   "metadata": {},
   "source": [
    "# train test 분할 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a475b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "#declear path to your mnist data folder\n",
    "img_path = '/app/FL_Mnist/Mnist'\n",
    "\n",
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=10000)\n",
    "\n",
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530dba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00392157 0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00784314 0.01960784 0.01568627 0.01176471 0.01176471 0.01176471\n",
      " 0.01960784 0.02745098 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01176471 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01176471 0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00392157 0.00784314\n",
      " 0.00784314 0.00392157 0.00392157 0.00784314 0.         0.01568627\n",
      " 0.01568627 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00392157 0.00392157 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01176471 0.01176471\n",
      " 0.00392157 0.         0.         0.         0.01176471 0.03137255\n",
      " 0.05882353 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.0627451  0.11764706 0.16078431 0.24313725 0.09019608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.00392157 0.01176471 0.0627451\n",
      " 0.1254902  0.47843137 0.54509804 0.7254902  0.75686275 0.91764706\n",
      " 0.94901961 1.         0.95294118 0.02352941 0.         0.02352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05490196 0.         0.0627451\n",
      " 0.48627451 0.34117647 0.67058824 0.94509804 0.98039216 1.\n",
      " 0.91764706 1.         0.8627451  0.81176471 1.         0.94509804\n",
      " 0.47843137 0.         0.00392157 0.01568627 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.02745098 0.69019608 0.94901961 0.91372549\n",
      " 1.         1.         1.         0.92941176 0.28235294 0.08627451\n",
      " 0.03921569 0.27058824 0.7372549  0.11372549 0.05098039 0.\n",
      " 0.00784314 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03137255 0.16078431\n",
      " 0.65490196 0.97647059 1.         0.59607843 0.1372549  0.1372549\n",
      " 0.86666667 0.44705882 0.10588235 0.01568627 0.         0.\n",
      " 0.04313725 0.03137255 0.         0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.80784314 0.98431373 1.\n",
      " 1.         0.91764706 1.         0.45490196 0.24705882 0.41176471\n",
      " 0.         0.00784314 0.01568627 0.         0.02352941 0.\n",
      " 0.02352941 0.02352941 0.         0.02352941 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.15294118 0.77254902 0.70588235 0.49803922 0.29803922 0.16470588\n",
      " 0.12941176 0.89411765 0.80784314 0.66666667 0.01960784 0.\n",
      " 0.01568627 0.04313725 0.         0.         0.01176471 0.\n",
      " 0.         0.05098039 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00784314\n",
      " 0.07843137 0.         0.05490196 0.         0.15294118 0.8627451\n",
      " 0.98039216 0.59215686 0.         0.03921569 0.01176471 0.\n",
      " 0.05490196 0.02745098 0.         0.         0.         0.02352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00392157 0.         0.         0.03137255\n",
      " 0.         0.04313725 0.01960784 0.25098039 0.98823529 0.17254902\n",
      " 0.         0.05490196 0.         0.00392157 0.         0.\n",
      " 0.         0.03921569 0.         0.         0.         0.00392157\n",
      " 0.         0.         0.05882353 0.         0.         0.02352941\n",
      " 0.         0.04313725 0.         0.         0.05098039 0.\n",
      " 0.         0.92941176 1.         0.05098039 0.         0.\n",
      " 0.         0.00392157 0.01568627 0.         0.         0.\n",
      " 0.         0.         0.03921569 0.         0.01568627 0.04313725\n",
      " 0.         0.03529412 0.         0.         0.08235294 0.\n",
      " 0.07058824 0.01176471 0.         0.         0.07843137 0.98039216\n",
      " 0.76862745 0.04313725 0.01176471 0.00392157 0.01960784 0.\n",
      " 0.01176471 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05098039 0.         0.14117647\n",
      " 0.51764706 0.43137255 0.0627451  0.08627451 0.         0.\n",
      " 0.         0.08627451 0.81568627 0.97254902 0.36470588 0.\n",
      " 0.         0.00784314 0.02745098 0.         0.02352941 0.\n",
      " 0.         0.         0.         0.         0.02352941 0.03921569\n",
      " 0.00392157 0.01568627 0.         0.82745098 1.         0.99215686\n",
      " 1.         0.83921569 0.78823529 0.76862745 0.75294118 0.72941176\n",
      " 0.97254902 0.7254902  0.0627451  0.         0.         0.00392157\n",
      " 0.02352941 0.         0.03529412 0.         0.         0.\n",
      " 0.         0.         0.         0.03137255 0.         0.\n",
      " 0.03137255 0.28235294 0.77254902 0.94509804 0.99215686 0.99607843\n",
      " 0.98431373 0.97647059 1.         1.         0.65098039 0.\n",
      " 0.         0.01568627 0.01568627 0.01176471 0.02352941 0.\n",
      " 0.02745098 0.         0.         0.         0.         0.\n",
      " 0.02352941 0.00784314 0.         0.13333333 0.         0.\n",
      " 0.05490196 0.64705882 0.99215686 0.94901961 1.         0.94901961\n",
      " 0.63921569 0.35686275 0.         0.01568627 0.         0.01176471\n",
      " 0.00784314 0.         0.00392157 0.         0.01960784 0.\n",
      " 0.         0.         0.         0.         0.         0.04705882\n",
      " 0.         0.         0.00392157 0.         0.03529412 0.00392157\n",
      " 0.25882353 0.37647059 0.05098039 0.07843137 0.05882353 0.\n",
      " 0.04705882 0.         0.         0.         0.         0.\n",
      " 0.         0.00784314 0.00784314 0.01176471 0.         0.\n",
      " 0.         0.         0.00392157 0.         0.         0.03529412\n",
      " 0.02745098 0.         0.         0.02352941 0.         0.03137255\n",
      " 0.02352941 0.         0.         0.01176471 0.         0.01960784\n",
      " 0.03529412 0.         0.01960784 0.02745098 0.         0.01568627\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[4564])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc121d46",
   "metadata": {},
   "source": [
    "# 클라이언트 -> 데이터 샤드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91df2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=2, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf0b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398ef6d",
   "metadata": {},
   "source": [
    "# 클라이언트 및 테스트 데이터 처리 및 일괄 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca553af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd942cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37800\n",
      "4200\n",
      "37800\n",
      "4200\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037cf41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 19:25:07.930606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 19:25:07.931026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 19:25:07.931396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-12 19:25:07.931823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.931862: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.931897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.931931: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.931963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.931998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.932030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/user/miniconda/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-04-12 19:25:07.932037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-12 19:25:07.932273: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5888697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(test_batched))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c9c6b",
   "metadata": {},
   "source": [
    "#  (Multi Layer Perceptron) 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "258d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc4cf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 3\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b54e0",
   "metadata": {},
   "source": [
    "# 모델 집계 (연동 평균)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cc981a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클라이언트 데이터의 스케일링 계수 \n",
    "# def weight_scalling_factor(clients_trn_data, client_name):\n",
    "#     client_names = list(clients_trn_data.keys())\n",
    "#     #get the bs\n",
    "#     bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "#     #first calculate the total training data points across clinets\n",
    "#     global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "#     # get the total number of data points held by a client\n",
    "#     local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "# #     print(\"local_count: \", local_count)\n",
    "# #     print(\"global_count: \", global_count)\n",
    "#     return local_count/global_count\n",
    "\n",
    "# # 위에서 구한 스케일링 계수를 이용해 스케일된 weight 구하기\n",
    "# def scale_model_weights(weight, scalar):\n",
    "#     '''function for scaling a models weights'''\n",
    "#     weight_final = []\n",
    "#     steps = len(weight)\n",
    "#     for i in range(steps):\n",
    "#         weight_final.append(scalar * weight[i])\n",
    "#     return weight_final\n",
    "\n",
    "\n",
    "# # \n",
    "# def sum_scaled_weights(scaled_weight_list):\n",
    "#     '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "#     avg_grad = list()\n",
    "#     #get the average grad accross all client gradients\n",
    "#     for grad_list_tuple in zip(*scaled_weight_list):\n",
    "#         layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "#         avg_grad.append(layer_mean)\n",
    "        \n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "# def test_model(X_test, Y_test,  model, comm_round):\n",
    "#     cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#     #logits = model.predict(X_test, batch_size=100)\n",
    "#     logits = model.predict(X_test)\n",
    "#     loss = cce(Y_test, logits)\n",
    "#     acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "#     print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "#     return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b28ec",
   "metadata": {},
   "source": [
    "# 연합 모델 교육"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97c83cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 49.452% | global_loss: 2.2667386531829834\n",
      "comm_round: 1 | global_acc: 69.643% | global_loss: 2.210482120513916\n",
      "comm_round: 2 | global_acc: 74.738% | global_loss: 2.138488292694092\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)\n",
    "global_weights = global_model.get_weights()\n",
    "\n",
    "HOST = '203.230.104.73'\n",
    "PORT = '9004'\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "server_socket.bind((HOST, PORT))\n",
    "\n",
    "# 서버가 클라이언트의 접속을 허용하도록 합니다. \n",
    "server_socket.listen()\n",
    "\n",
    "# accept 함수에서 대기하다가 클라이언트가 접속하면 새로운 소켓을 리턴합니다. \n",
    "client_socket, addr = server_socket.accept()\n",
    "\n",
    "# 접속한 클라이언트의 주소입니다.\n",
    "print('Connected by', addr)\n",
    "\n",
    "\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    while True:\n",
    "\n",
    "    # 클라이언트가 보낸 메시지를 수신하기 위해 대기합니다. \n",
    "    data = client_socket.recv(1024)\n",
    "\n",
    "    # 빈 문자열을 수신하면 루프를 중지합니다. \n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    # 수신받은 문자열을 출력합니다.\n",
    "    print('Received from', addr, data.decode())\n",
    "\n",
    "    # 받은 문자열을 다시 클라이언트로 전송해줍니다.(에코) \n",
    "    client_socket.sendall(data)\n",
    "\n",
    "    \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0c353d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d85293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
